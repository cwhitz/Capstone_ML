{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadband Growth Across the US\n",
    "## Machine Learning\n",
    "### Chester Hitz | Springboard Data Science Career Track | Capstone I\n",
    "\n",
    "The goal of this final component of my capstone is to apply what I have discovered thus far in my project and apply it to build a predictive model using tools from sci-kit learn. I will build and evaluate several models, then try to establish a deeper understanding of my model's strengths and weaknesses as well as the real-world applications of my results.\n",
    "\n",
    "### Work thus far\n",
    "I started this project with my data wrangling component, where I assembled a DataFrame consisting of 13k rows, each one representing a county in a particular year across a four year period (2011-2015) with a target variable. The target variable is a number corresponding to a category for \"residential fixed Internet access connections per 1,000 households by county for both service over 200 kbps in at least one direction and service at least 10 Mbps down / 1 Mbps up.\". This is the BSC, or broadband subscription category. The categories are listed as follows, with x representing connections per 1,000 households:\n",
    "* 0: Zero \n",
    "* 1: Zero < x <= 200 \n",
    "* 2: 200 < x <= 400 \n",
    "* 3: 400 < x <= 600 \n",
    "* 4: 600 < x <= 800 \n",
    "* 5: 800 < x\n",
    "\n",
    "I then populated this DataFrame further with various features pulled from the census API and other sources to complete my DataWrangling component. In my Data Storytelling and Inferential Statistics modules, I examined the data and found 1) that BSC did move upward over the study period and 2) that economic factors were the most closely correlated to BSC.\n",
    "\n",
    "In this final module, I will build on those previous units to construct a predictive model. Though I am technically predicting for a category, the category is essentially bins for numerical data, so a scikit-learn LinearRegression model will be used in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin by importing all relevant libraries & previous data\n",
    "# standard data analysis libraries...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ... and various scikit-learn functions\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Imputer, normalize\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# import relevant data\n",
    "broadband = pd.read_csv('broadband.csv', dtype={'county_fips':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first step here is preprocessing my data for sci-kit learn's algorithms: Dropping unnecessary categorical features and the target variable and imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unnecessary features, impute NaN values, and isolate the features from the target in a new DataFrame\n",
    "lr_bb = broadband.drop(['Unnamed: 0', 'state', 'statename'], axis=1)\n",
    "df_features = lr_bb.drop('BSC', axis = 1)\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "df_features = imp.fit_transform(df_features)\n",
    "\n",
    "# split the data 80/20 between training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, lr_bb.BSC, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step is designing a function that will allow me to quickly preprocess my data and run it through a selected model to produce metrics for comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a function to train and test different DataFrames\n",
    "\n",
    "# coefficient printer function from: http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\n",
    "def pretty_print_linear(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \"\\n + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)\n",
    "def preprocess(df):\n",
    "    global X_train, X_holdout, y_train, y_holdout, df_features\n",
    "    df_features = df.drop('BSC',axis = 1)\n",
    "    df_features = imp.fit_transform(df_features)\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(df_features, df.BSC, test_size = .2, random_state=24)\n",
    "\n",
    "def LR_Pipeline(df, model, coef_print=True, return_df=False):\n",
    "    # preprocess & segment data\n",
    "    preprocess(df)\n",
    "    \n",
    "    # fit data to a linear regression model\n",
    "    scores = cross_val_score(model,X_train,y_train, cv=5)\n",
    "    \n",
    "    # evaluate RMSE on holdout set\n",
    "    lr = model.fit(X_train,y_train)\n",
    "    BSC_predicted = lr.predict(X_holdout)\n",
    "    BSC_predicted[BSC_predicted > 5] = 5\n",
    "    global holdout_rmse\n",
    "    holdout_rmse = np.sqrt(mean_squared_error(BSC_predicted, y_holdout))\n",
    "    \n",
    "    print('Training test CV scores:{}'.format(scores))\n",
    "    print('Hold-out RMSE: {}'.format(holdout_rmse))\n",
    "    if coef_print==True:\n",
    "        print(\"Linear model:\\n\", pretty_print_linear(lr.coef_, names=list(df.columns), sort=True))\n",
    "    print('--------------------')\n",
    "    \n",
    "    if return_df==True: \n",
    "        df_copy = df\n",
    "        df_copy.loc[:,'Predicted'] = lr.predict(df_features)\n",
    "        df_copy.loc[:,'Residual'] = df['BSC'] - df['Predicted']\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third step, performed below, is model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Simple linear regression models the relationship between the magnitude of one variable and that of a second. Regression goes beyond the previous analysis of regression that measured the strength of the association between variables, and tries to quantify and utilize that relationship to predict.\n",
    "\n",
    "In the code below, my LR_Pipeline function is utilized on the dataset, and the RMSE score is provided along with a listing of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test CV scores:[ 0.52678326  0.50015789  0.48937864  0.52120831  0.50984972]\n",
      "Hold-out RMSE: 0.5692135410980559\n",
      "Linear model:\n",
      " 0.117 * Year\n",
      " + 0.036 * EmploymentRate\n",
      " + 0.036 * NoEnglish\n",
      " + -0.027 * AgJobs\n",
      " + 0.023 * Families\n",
      " + 0.021 * BornInState\n",
      " + -0.019 * ArmedForcesPer\n",
      " + -0.017 * AptBuildings\n",
      " + -0.016 * BSC\n",
      " + 0.014 * InformationJobsPer\n",
      " + 0.011 * MobileHomes\n",
      " + -0.007 * HighSchoolGrads\n",
      " + -0.004 * PublicTrans\n",
      " + -0.004 * colleges\n",
      " + 0.003 * InformationJobs\n",
      " + 0.0 * Population\n",
      " + 0.0 * TractArea\n",
      " + 0.0 * Unemployed\n",
      " + -0.0 * NumberofFirms\n",
      " + -0.0 * PctBachelors\n",
      " + -0.0 * Annual Payroll\n",
      " + 0.0 * HomePrice\n",
      " + 0.0 * TotalHouseholds\n",
      " + -0.0 * Rural_Pct\n",
      " + 0.0 * Income\n",
      " + -0.0 * county_fips\n",
      " + -0.0 * SameHouseResidence\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "LR_Pipeline(lr_bb, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting my data to the most generalized linear regression model produces an RMSE of .57, meaning that is the average error from actual BSC is about half. This is not awful, but improvements can be made in two ways: 1) a simpler model, and 2) a lower RMSE score indicating more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "In regression, it is always best to take the simplest approach possible and use the least number of predictor variables when possible.  Ridge regression is a linear model that simplifies Linear Regression by making the magnitude of the coefficients as small as absolutely possible while still predicting well. This will provide us with a model that predicts equally well but relies on less features to make that prediction, while also preventing overfitting.\n",
    "\n",
    "Unlike linear regression, there are hyperparameters that need to be chosen in Ridge Regression. The hyperparameter in this case is alpha, and it is selected below via a search through a range of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.001\n",
      "Training test CV scores:[ 0.52678326  0.50015789  0.48937864  0.52120831  0.50984972]\n",
      "Hold-out RMSE: 0.5692135409735029\n",
      "--------------------\n",
      "alpha: 0.01\n",
      "Training test CV scores:[ 0.52678327  0.50015789  0.48937864  0.5212083   0.50984972]\n",
      "Hold-out RMSE: 0.569213539871282\n",
      "--------------------\n",
      "alpha: 0.1\n",
      "Training test CV scores:[ 0.52678328  0.50015791  0.48937866  0.52120829  0.50984974]\n",
      "Hold-out RMSE: 0.5692135288496882\n",
      "--------------------\n",
      "alpha: 1\n",
      "Training test CV scores:[ 0.52678338  0.50015805  0.48937887  0.52120814  0.50984995]\n",
      "Hold-out RMSE: 0.5692134186947851\n",
      "--------------------\n",
      "alpha: 10\n",
      "Training test CV scores:[ 0.52678439  0.50015941  0.48938092  0.52120661  0.50985197]\n",
      "Hold-out RMSE: 0.5692123232443912\n",
      "--------------------\n",
      "alpha: 100\n",
      "Training test CV scores:[ 0.52679305  0.5001712   0.48939803  0.52119001  0.50987051]\n",
      "Hold-out RMSE: 0.5692019723248875\n",
      "--------------------\n",
      "alpha: 1000\n",
      "Training test CV scores:[ 0.52674824  0.50012631  0.4893179   0.52090878  0.50991093]\n",
      "Hold-out RMSE: 0.5691170048275637\n",
      "--------------------\n",
      "alpha: 10000\n",
      "Training test CV scores:[ 0.52122952  0.49397791  0.48284705  0.51403937  0.5048705 ]\n",
      "Hold-out RMSE: 0.5708363093146438\n",
      "--------------------\n",
      ">>> Best alpha: 1000\n"
     ]
    }
   ],
   "source": [
    "current_min = 1\n",
    "best_alpha = None\n",
    "\n",
    "for i in [.001,.01,.1,1,10,100,1000,10000]:\n",
    "    print('alpha:', i)\n",
    "    LR_Pipeline(lr_bb, Ridge(alpha=i), coef_print=False)\n",
    "    if holdout_rmse < current_min: \n",
    "        current_min = holdout_rmse\n",
    "        best_alpha = i\n",
    "        \n",
    "print('>>> Best alpha:', best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffences in RMSE are very small, but we are not necessairily searching for that here. What we are really looking for is a way to simplify our model. In order to do so, I want to take a look at the coefficients with a Ridge model with that optimized alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test CV scores:[ 0.52674824  0.50012631  0.4893179   0.52090878  0.50991093]\n",
      "Hold-out RMSE: 0.5691170048275637\n",
      "Linear model:\n",
      " 0.112 * Year\n",
      " + 0.034 * EmploymentRate\n",
      " + 0.034 * NoEnglish\n",
      " + -0.026 * AgJobs\n",
      " + 0.023 * Families\n",
      " + 0.021 * BornInState\n",
      " + -0.018 * ArmedForcesPer\n",
      " + -0.016 * BSC\n",
      " + -0.014 * AptBuildings\n",
      " + 0.014 * InformationJobsPer\n",
      " + 0.01 * MobileHomes\n",
      " + -0.007 * HighSchoolGrads\n",
      " + -0.004 * PublicTrans\n",
      " + -0.004 * colleges\n",
      " + 0.003 * InformationJobs\n",
      " + 0.0 * Population\n",
      " + 0.0 * TractArea\n",
      " + 0.0 * Unemployed\n",
      " + -0.0 * NumberofFirms\n",
      " + -0.0 * PctBachelors\n",
      " + 0.0 * HomePrice\n",
      " + -0.0 * Annual Payroll\n",
      " + 0.0 * TotalHouseholds\n",
      " + -0.0 * Rural_Pct\n",
      " + 0.0 * Income\n",
      " + -0.0 * county_fips\n",
      " + -0.0 * SameHouseResidence\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "LR_Pipeline(lr_bb, Ridge(alpha=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has determined which coefficients can be used to make accurate predictions, and which we can eliminate. We can test this by dropping features which were given coefficients equal to zero in the initial run of our Ridge regression test, and then testing that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test CV scores:[ 0.47606171  0.42921875  0.42982885  0.45819561  0.45354681]\n",
      "Hold-out RMSE: 0.602799507572941\n",
      "Linear model:\n",
      " 0.115 * Year\n",
      " + 0.05 * EmploymentRate\n",
      " + 0.05 * NoEnglish\n",
      " + 0.032 * BornInState\n",
      " + -0.03 * AptBuildings\n",
      " + 0.03 * MobileHomes\n",
      " + -0.018 * HighSchoolGrads\n",
      " + -0.01 * PublicTrans\n",
      " + -0.004 * InformationJobs\n",
      " + -0.0 * AgJobs\n",
      " + -0.0 * InformationJobsPer\n",
      " + 0.0 * county_fips\n",
      " + 0.0 * BSC\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "zero_coef_features = ['Population','TractArea','PctBachelors','Unemployed','SameHouseResidence',\n",
    "                        'HomePrice','TotalHouseholds','NumberofFirms','Annual Payroll','Income','Rural_Pct',\n",
    "                      'Families','ArmedForcesPer','colleges']\n",
    "test_df = lr_bb.drop(zero_coef_features, axis=1)\n",
    "LR_Pipeline(test_df, Ridge(alpha=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV scores went down slightly and the RMSE increased just a bit, but it is still impressive how well the model performs with just 10 of the original 24 variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach in contrast to Ridge regression to simplify our model is iterative feature selection. This method starts with a fully built linear model, then iteratively eliminates features from the model depending on their scores, also known as Recursive Feature Elimination (RFE). It should be noted that this is a very computationally expensive method given the large amount of cross validation performed, but my dataset here is small enough that it can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an effective means of removing unnecessary features from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_dict = {}\n",
    "\n",
    "for i in reversed(range(1, lr_bb.shape[1])):\n",
    "    # preprocess & segment data\n",
    "    preprocess(lr_bb)\n",
    "\n",
    "    # fit data to a linear regression model\n",
    "    lm = LinearRegression()\n",
    "    Selector = RFE(lm, n_features_to_select=i, step=1, verbose=0)\n",
    "    reduced_lm = Selector.fit(X_train,y_train)\n",
    "\n",
    "    # use it to predict\n",
    "    BSC_predicted = reduced_lm.predict(X_holdout)\n",
    "    BSC_predicted[BSC_predicted > 5] = 5\n",
    "\n",
    "    # save\n",
    "    rmse = np.sqrt(mean_squared_error(BSC_predicted, y_holdout))\n",
    "    n_features_dict[i] = rmse\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119f3320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_features_dict.keys(),n_features_dict.values())\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RFE Assessment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the figure above, we can see at x=11 RMSE falls sharply. This would be a number of features and is comporable to the number of features I reduced to in my Ridge exploration. Below I restrict the the dataset to the top 11 selected by RFE features and test for RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BSC' 'Rural_Pct' 'PctBachelors' 'InformationJobs' 'BornInState'\n",
      " 'HighSchoolGrads' 'MobileHomes' 'NoEnglish' 'InformationJobsPer'\n",
      " 'ArmedForcesPer' 'Unemployed']\n"
     ]
    }
   ],
   "source": [
    "preprocess(lr_bb)\n",
    "\n",
    "# fit data to a linear regression model\n",
    "lm = LinearRegression()\n",
    "Selector = RFE(lm, n_features_to_select=11, step=1, verbose=0)\n",
    "reduced_lm = Selector.fit(X_train,y_train)\n",
    "\n",
    "RFE_features = np.array(list(lr_bb.columns)[1:])\n",
    "RFE_features = RFE_features[np.array(reduced_lm.support_)]\n",
    "print(RFE_features)\n",
    "\n",
    "RFE_bb = lr_bb[RFE_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test CV scores:[ 0.46729722  0.4374322   0.42919033  0.45973047  0.45703664]\n",
      "Hold-out RMSE: 0.5995774503721649\n",
      "Linear model:\n",
      " 0.035 * Rural_Pct\n",
      " + 0.03 * NoEnglish\n",
      " + 0.03 * PctBachelors\n",
      " + 0.027 * BornInState\n",
      " + -0.024 * MobileHomes\n",
      " + -0.021 * ArmedForcesPer\n",
      " + 0.01 * InformationJobsPer\n",
      " + -0.008 * HighSchoolGrads\n",
      " + -0.006 * InformationJobs\n",
      " + -0.005 * BSC\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "LR_Pipeline(RFE_bb, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Like ridge regression, we can hold a steady RMSE with a significantly reduced number of features using this method.\n",
    "\n",
    "What is interesting in comparing these two methods of feature engineering is which features were selected in each case. Ridge regression operates in a regularized manner, so that the features initially observed to be highly correlated with the target variable do not always end up having significant coefficients since the model is assessed and built in a totally different way. In contrast, RFE selects features that are less surprising given my findings in examing correlations between features and the target. \n",
    "\n",
    "Are there ways to further improve the model now that we have simplified it a bit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful improvement was segmenting the data based on its real-world origins. Each record is a county in a particular year, and we are trying to predict based on various demographic factors collected by the census bureau. In order to improve RMSE here, I segmented the data into three distinct DataFrames representing rural, semirural, and urban counties and trained a regression model on each (based on definitions from the census bureau). I did this because the various features operate differently at different levels of rurality, for example I discovered that income matters much less as rurality increases. The result is better trained models that produce lower RMSE scores overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban. n: 6285\n",
      "Training test CV scores:[ 0.59994266  0.55962233  0.53734139  0.5558144   0.55480752]\n",
      "Hold-out RMSE: 0.45361766528005404\n",
      "--------------------\n",
      "Semirural. n: 5900\n",
      "Training test CV scores:[ 0.43493227  0.40287148  0.39723622  0.43288445  0.44389484]\n",
      "Hold-out RMSE: 0.5726702272784939\n",
      "--------------------\n",
      "Rural. n: 3493\n",
      "Training test CV scores:[ 0.34316487  0.3552628   0.28667418  0.39126906  0.30462013]\n",
      "Hold-out RMSE: 0.6708333363533827\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# segment the DataFrames and train individual linear regression models on each\n",
    "urban_bb = lr_bb[lr_bb['Rural_Pct'] < 50]\n",
    "semirural_bb = lr_bb[(lr_bb['Rural_Pct'] > 50) & (broadband['Rural_Pct'] < 99.9)]\n",
    "rural_bb = lr_bb[lr_bb['Rural_Pct'] == 100]\n",
    "\n",
    "a=1000\n",
    "\n",
    "print('Urban. n:', urban_bb.shape[0])\n",
    "LR_Pipeline(urban_bb,LinearRegression(), coef_print=False)\n",
    "print('Semirural. n:', semirural_bb.shape[0])\n",
    "LR_Pipeline(semirural_bb,LinearRegression(), coef_print=False)\n",
    "print('Rural. n:', rural_bb.shape[0])\n",
    "LR_Pipeline(rural_bb,LinearRegression(), coef_print=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a lot to unpack here. Overall, I can predict urban and semirural counties fairly well and better than when the model is trained on the entire dataset. Urban counties in particular have an RMSE of .45, meaning that the average prediction is only off by .45. Considering that urban counties are the most numerous, this is good news. However, the more rural the county is, generally the worse off the model gets at predicting BSC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "At this point it is good to ask two questions: What exaclty has been built in this and how is it useful? Reflecting again on the real-world origins and goals of this project, it is very useful to examine the residuals of the prediction since they show which counties would be best for further expansion by telecom companies. A histogram of the residuals (actual - predicted) is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test CV scores:[ 0.52678326  0.50015789  0.48937864  0.52120831  0.50984972]\n",
      "Hold-out RMSE: 0.5692135410980559\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoAkdcoAkKRO/T96EvFhcQ0BNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a899b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resid_df = LR_Pipeline(lr_bb, LinearRegression(), coef_print=False, return_df=True)\n",
    "plt.hist(resid_df.Residual)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_df[resid_df['Year']==2011].to_csv('resid_map_2011.csv')\n",
    "resid_df[resid_df['Year']==2015].to_csv('resid_map_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project is based on geographical units, it makes sense to map our residuals to see if there are any patterns. Below are two maps of the residuals for 2015 and 2011, with the coloration corresponding to the number of standard deviations from the mean of the residuals for that particular year. Put simply, red counties are overpredicted, and blue counties are underpredicted.\n",
    "\n",
    "These maps achieve the objective of this project: they identify areas that are underserved in home broadband subscription based on rigorous statistical calculation of factors that lead to broadband subscription rates. It should be noted that they are results from the generalized linear regression model rather than the segmented models from the \"Further Optimization\" section, but they still provide us, and telecom companies that are the theoretical consumers of this project, useful information. For example, it looks like in the most recent map that Arkansas, Mississippi and Lousiana in the south are the most underserved and could benefit from market expansion. Same with some areas of southern Virginia as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2015 Residuals](2015_r_map.png)\n",
    "<center> **2015 Residuals** </center>\n",
    "\n",
    "![2011 Residuals](2011_r_map.png)\n",
    "<center> **2011 Residuals** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has successfully provided a predictive model for BSC, achieving an RMSE of .57 in the generalized model and as low as .45 for subsets of the data. Data was collected from a variety of sources, cleaned, and arranged. EDA was undertaken to determine general patterns in the data, as well as discovery of highly correlated features. In this final unit, a predictive linear regression model was built, simplified in two different ways, and used to produce a valuable map of residuals.\n",
    "\n",
    "Further research could be undertaken to understand the high variance and poor predictability of in rural counties. Development of a feature that performs particularly well in rural counties would be very valuable in reducing overall RMSE. Additionally, spatial interpolation techniques could be used to improve accuracy of the predictions even further. I may return to this project in the future to do that, but will leave it for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
