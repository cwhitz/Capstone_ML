{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Broadband Growth Across the US ðŸ’»\n",
    "## Data Wrangling\n",
    "### Chester Hitz | Springboard Data Science Career Track | Capstone I\n",
    "\n",
    "The goal of this section of my capstone project is to capture and arrange the data I will need to determine the factors that lead to the growth of broadband subscription at the county level across the United States. \n",
    "\n",
    "-----------------\n",
    "### The Target Data\n",
    "\n",
    "The first step in this process is to capture the most important data: rates of broadband subscription by county. A county is a political unit below the state. The data for this is coming from \"Form 477 County Data on Internet Access Services\", a dataset provided by the Federal Communications Commision (FCC). In this dataset, each county is given a number corresponding to a category for \"residential fixed Internet access connections per 1,000 households by county for both service over 200 kbps in at least one direction and service at least 10 Mbps down / 1 Mbps up.\". I will refer to this as BSC, or broadband subscription category. The categories are listed as follows, with x representing connections per 1,000 households:\n",
    "* 0: Zero \n",
    "* 1: Zero < x <= 200 \n",
    "* 2: 200 < x <= 400 \n",
    "* 3: 400 < x <= 600 \n",
    "* 4: 600 < x <= 800 \n",
    "* 5: 800 < x\n",
    "\n",
    "The data is provided in csv format from the <a href= https://www.fcc.gov/general/form-477-census-tract-data-internet-access-services> FCC website </a>. The years I want to capture are 2011 - 2016, and I want to create a dataframe that captures all the years of all counties as rows with a column for each year showing the broadband category that year. The aim here is to create a dataset that is easily readable for supervised machine learning, with this \"broadband category\" variable being the target to predict.\n",
    "\n",
    "I do this below by first importing the 2011 data and using as the basis to merge additional years based on a common key of county IDs. I found the cleanest way to do this on import was to zip up a dictionary with keys set to county_ids and values set to the BSC for that year. Finally, the entire DataFrame is melted around that variable, lengthening the entire dataset by a factor of five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state statename county_fips  Year  BSC\n",
      "0      1   Alabama       01001  2011    4\n",
      "1      1   Alabama       01003  2011    4\n",
      "2      1   Alabama       01005  2011    3\n",
      "3      1   Alabama       01007  2011    2\n",
      "4      1   Alabama       01009  2011    3\n",
      "       state statename county_fips  Year  BSC\n",
      "15710     56   Wyoming       56037  2015    4\n",
      "15711     56   Wyoming       56039  2015    5\n",
      "15712     56   Wyoming       56041  2015    5\n",
      "15713     56   Wyoming       56043  2015    4\n",
      "15714     56   Wyoming       56045  2015    3\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15715 entries, 0 to 15714\n",
      "Data columns (total 5 columns):\n",
      "state          15715 non-null int64\n",
      "statename      15715 non-null object\n",
      "county_fips    15715 non-null object\n",
      "Year           15715 non-null object\n",
      "BSC            15715 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 613.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Initial read of 2011 data; removal of unnecessary columns; dropping of counties not in 50 states (US territories)\n",
    "broadband = pd.read_csv('hs_countydata_dec_2011.csv', header=0, encoding='latin-1', \n",
    "                        dtype={'county_fips':str, 'rfc_per_1000_hhs':int})\n",
    "\n",
    "# county_fips,state,county,statename,countyname,rfc_per_1000_hhs\n",
    "\n",
    "broadband = broadband[['state','statename','county_fips','rfc_per_1000_hhs']]\n",
    "broadband = broadband[broadband['state'] < 57] #this drops US territories (Guam etc)\n",
    "\n",
    "years = ['2011','2012','2013','2014','2015']\n",
    "\n",
    "for year in years:\n",
    "    yearFile = 'hs_countydata_dec_' + year + '.csv'\n",
    "    year_df = pd.read_csv(yearFile, header=0, encoding='latin-1', dtype={'county_fips':str, 'rfc_per_1000_hhs':int})\n",
    "    year_dict = dict(zip(year_df.county_fips, year_df.rfc_per_1000_hhs))\n",
    "    broadband[year] = broadband['county_fips'].map(year_dict)\n",
    "    \n",
    "broadband.set_index('county_fips')\n",
    "    \n",
    "broadband = broadband.melt(id_vars=['state','statename','county_fips'], value_vars = years, var_name='Year', value_name = 'BSC')\n",
    "print(broadband.head(5))\n",
    "print(broadband.tail(5))\n",
    "print(broadband.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors\n",
    "\n",
    "#### Colleges\n",
    "\n",
    "Before moving onto dynamic variables for each year, I would like to add a factor that counts the number of colleges and universities in the county, as I suspect that having a college within the county could be a strong predictor of broadband subscription. I intially tried to do this by running a loop that pinged the Census Geocoder API for the county of each individual college, but that ended up overloading the Census geocoder and was taking hours to process.\n",
    "\n",
    "Instead, I do this by downloading data with geographical data for all the colleges in the US <a href= https://www.sciencebase.gov/catalog/item/4f4e4acee4b07f02db67fb39> collected here</a>, containing the county ID for each college (the .dbf file was processed externally in QGIS to turn it into a .csv). The unique county IDs are counted with a Counter object, which creates a dictionary containing counts for each county. The dataframe is then  Tract IDs are added to a list, which is passed through a counter object / dictionary, which is then mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# import data, select only qualifying schools, create Counter dict\n",
    "colleges = pd.read_csv('colleges.csv', dtype={'COFIPS':str})\n",
    "colleges = colleges[colleges['NAICS_DESC']== 'Colleges, Universities, and Professional Schools']\n",
    "college_dict = Counter(list(colleges.COFIPS))\n",
    "\n",
    "broadband['colleges'] = broadband['county_fips'].map(college_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rural Percentage\n",
    "\n",
    "One useful metric provided by the census bureau at the county level is the percentage of the population that can be classified as rural. The exact formulation of this is detailed <a href='https://www2.census.gov/geo/pdfs/reference/ua/Defining_Rural.pdf'>here</a>. For my purposes, I downloaded an Excel file, converted it to csv, and added in a similar fashion to the other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rural_df = pd.read_csv('county_rural.csv', dtype={'2015 GEOID':str})\n",
    "rural_dict = dict(zip(rural_df['2015 GEOID'], rural_df['2010 Census \\rPercent Rural']))\n",
    "broadband['Rural_Pct'] = broadband['county_fips'].map(rural_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Additional Factors\n",
    "\n",
    "Adding additional factors from the census bureau is an interesting challenge. Thousands of data series are available as CSVs and other formats from the census website, but I decided the most dynamic way to add additional data to my dataframe would be through a function that: 1) Submits a request to the Census Bureau API, 2) Recieves the JSON response and parses it, 3) creates a dictionary with the relevant data in the form of: {county_id: value}, 4) maps that dictionary to a new column. This allows for more dynamic addition of factors to the dataframe without having to scrub and clean additional CSVs as they come in.\n",
    "\n",
    "Two functions to do this are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Defines a basic function to pull census data depending on parameters given\n",
    "def census_pull(url, parameter):\n",
    "    payload = {'get': parameter,\n",
    "                   'for':'county:*',\n",
    "                   'in': 'state:*',\n",
    "                   'key': '83a87cce9c801337304138c3da4ee7e290ccb204'}\n",
    "    resp = requests.get(url, params = payload)\n",
    "    global data\n",
    "    data = json.loads(resp.text)\n",
    "\n",
    "# Main function to pull data and apply it to dataframe.\n",
    "def census_fill(df, url, parameter, parameter_name, year, dtype=float):\n",
    "    hold_dict = {}\n",
    "    # small modification to the url string to access data in 2011\n",
    "    if year == '2011': mod2011 = '/acs'\n",
    "    else: mod2011 = ''\n",
    "    full_url = url.format(str(year), mod2011)\n",
    "    census_pull(full_url, parameter)\n",
    "    for item in data[1:]:\n",
    "        hold_dict[str(item[1])+str(item[2])] = item[0]\n",
    "    df[parameter_name] = df['county_fips'].map(hold_dict)\n",
    "    df[parameter_name] = pd.to_numeric(df[parameter_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell collects and organizes the data into the dataframe using the two functions above. The variables here are not guessed a priori, but rather after a few rounds of testing for correlation against the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23 21:20:43.776123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChesterHitz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/ChesterHitz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 collection complete.\n",
      "2012 collection complete.\n",
      "2013 collection complete.\n",
      "2014 collection complete.\n",
      "2015 collection complete.\n",
      "End: 0:02:27.415639\n"
     ]
    }
   ],
   "source": [
    "metastartTime = datetime.now()\n",
    "print(metastartTime)\n",
    "\n",
    "acs_detail = {'Population':'B01001_001E',\n",
    "              'Income':'B06011_001E',\n",
    "             'HomePrice':'B25077_001E'}\n",
    "acs_summary = {'Families': 'DP02_0005E',\n",
    "               'PctBachelors':'DP02_0064PE',\n",
    "               'EmploymentRate':'DP03_0026PE',\n",
    "              'InformationJobs':'DP03_0039PE',\n",
    "              'AgJobs':'DP03_0033PE',\n",
    "              'PublicTrans': 'DP03_0021PE',\n",
    "              'BornInState':'DP02_0089PE',\n",
    "              'HighSchoolGrads':'DP02_0066PE',\n",
    "              'MobileHomes':'DP04_0014PE',\n",
    "              'AptBuildings':'DP04_0013PE',\n",
    "              'NoEnglish':'DP02_0121PE',\n",
    "              'InformationJobsPer':'DP03_0039PE',\n",
    "              'ArmedForcesPer':'DP03_0006PE',\n",
    "              'Unemployed':'DP03_0005PE',\n",
    "              'TotalHouseholds':'DP02_0001E',\n",
    "              'SameHouseResidence':'DP02_0079E'}\n",
    "CBP = {'Annual Payroll': 'PAYANN',\n",
    "      'NumberofFirms':'ESTAB'}\n",
    "\n",
    "new_broadband = pd.DataFrame()\n",
    "for year in years:\n",
    "    subset = broadband[broadband['Year'] == year]\n",
    "    for key in acs_detail:\n",
    "        census_fill(subset, 'https://api.census.gov/data/{}{}/acs5?', acs_detail.get(key), key, year)\n",
    "    for key in acs_summary:\n",
    "        census_fill(subset, 'https://api.census.gov/data/{}{}/acs5/profile?', acs_summary.get(key), key, year)\n",
    "    for key in CBP:\n",
    "        census_fill(subset, 'https://api.census.gov/data/{}/cbp?', CBP.get(key), key, year)\n",
    "    new_broadband = new_broadband.append(subset)\n",
    "    print(year + ' collection complete.')\n",
    "\n",
    "broadband = new_broadband\n",
    "print('End: ' + str(datetime.now() - metastartTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population Density\n",
    "\n",
    "One factor I would like to incorporate is population density (population per sq mile). This data is not available through the API but can be quickly calculated by reading a <a href = https://www.census.gov/geo/maps-data/data/gazetteer2015.html> gazetteer text file </a> and joining it in the dict/map method as the other variables. This is also a good opportunity to create a general purpose divider function for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gazetteer = pd.read_csv('2015_Gaz_counties_national.csv', header=0, dtype={'GEOID':str}, encoding='latin-1')\n",
    "gazetteer_dict = dict(zip(gazetteer['GEOID'], gazetteer['ALAND_SQMI']))\n",
    "broadband['TractArea'] = broadband['county_fips'].map(gazetteer_dict)\n",
    "\n",
    "def divider(df, row1, row2, newrow):\n",
    "    def subdivider(row):\n",
    "        if row[row2] != 0 and row[row2] != np.nan and row[row1] != 0 and row[row1] != np.nan:\n",
    "            return float(row[row1]) / float(row[row2])\n",
    "        else: return 0\n",
    "    df[newrow] = df.apply(subdivider, axis=1)\n",
    "    \n",
    "divider(broadband, 'Population','TractArea','PopDensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we can see we do have a number of null values. Further, looking over intermediate .csvs produced I found a number of values of -666666666, which is the census value for null. I replaced those values to be null, then took a look at the NaN values per column. The factor Income has by far the least number of NaN values. I did a quick analysis to see that removing Income would in fact also greatly reduce the number of NaN values overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "broadband = broadband.replace(-666666666, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total null: 870\n",
      "percent null: 0.05536111994909321\n",
      "total null: 32\n",
      "percent null: 0.0020362710785872817\n",
      "total null: 838\n",
      "percent null: 0.05343365427532998\n"
     ]
    }
   ],
   "source": [
    "print('total null:' , broadband.shape[0] - broadband.dropna().shape[0])\n",
    "print('percent null:' , 1-(broadband.dropna().shape[0]/broadband.shape[0]))\n",
    "\n",
    "test_drop = broadband.drop(['Income'], axis=1)\n",
    "\n",
    "print('total null:' , test_drop.shape[0] - test_drop.dropna().shape[0])\n",
    "print('percent null:' , 1-(test_drop.dropna().shape[0]/test_drop.shape[0]))\n",
    "\n",
    "factors = list(broadband.columns)\n",
    "factors.remove('Income')\n",
    "broadband = broadband.dropna(subset=(factors))\n",
    "\n",
    "print('total null:' , broadband.shape[0] - broadband.dropna().shape[0])\n",
    "print('percent null:' , 1-(broadband.dropna().shape[0]/broadband.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15683 entries, 0 to 15714\n",
      "Data columns (total 30 columns):\n",
      "state                 15683 non-null int64\n",
      "statename             15683 non-null object\n",
      "county_fips           15683 non-null object\n",
      "Year                  15683 non-null object\n",
      "BSC                   15683 non-null int64\n",
      "colleges              15683 non-null int64\n",
      "Rural_Pct             15683 non-null float64\n",
      "Population            15683 non-null float64\n",
      "Income                14845 non-null float64\n",
      "HomePrice             15683 non-null float64\n",
      "Families              15683 non-null float64\n",
      "PctBachelors          15683 non-null float64\n",
      "EmploymentRate        15683 non-null float64\n",
      "InformationJobs       15683 non-null float64\n",
      "AgJobs                15683 non-null float64\n",
      "PublicTrans           15683 non-null float64\n",
      "BornInState           15683 non-null float64\n",
      "HighSchoolGrads       15683 non-null float64\n",
      "MobileHomes           15683 non-null float64\n",
      "AptBuildings          15683 non-null float64\n",
      "NoEnglish             15683 non-null float64\n",
      "InformationJobsPer    15683 non-null float64\n",
      "ArmedForcesPer        15683 non-null float64\n",
      "Unemployed            15683 non-null float64\n",
      "TotalHouseholds       15683 non-null float64\n",
      "SameHouseResidence    15683 non-null float64\n",
      "Annual Payroll        15683 non-null float64\n",
      "NumberofFirms         15683 non-null float64\n",
      "TractArea             15683 non-null float64\n",
      "PopDensity            15683 non-null float64\n",
      "dtypes: float64(24), int64(3), object(3)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "broadband.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Conclusion\n",
    "\n",
    "In this step, I have collected and cleaned a number of factors that I can use to determine the growth of broadband. More importantly, I have created a number of easy to use functions to quickly add and manipulate data as I might need it going forward. It now simply takes one line of code to add additional factors to the dataframe. Next report will dive into the data collected and create visualizations and perform EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15683, 30)\n",
      "   state statename county_fips  Year  BSC  colleges  Rural_Pct  Population  \\\n",
      "0      1   Alabama       01001  2011    4         0  42.002162     53944.0   \n",
      "1      1   Alabama       01003  2011    4         2  42.279099    179523.0   \n",
      "2      1   Alabama       01005  2011    3         0  67.789635     27546.0   \n",
      "3      1   Alabama       01007  2011    2         0  68.352607     22746.0   \n",
      "4      1   Alabama       01009  2011    3         0  89.951502     57140.0   \n",
      "5      1   Alabama       01011  2011    2         0  51.374382     10877.0   \n",
      "6      1   Alabama       01013  2011    3         0  71.232157     20860.0   \n",
      "7      1   Alabama       01015  2011    3         1  33.696826    117614.0   \n",
      "8      1   Alabama       01017  2011    3         0  49.148034     34375.0   \n",
      "9      1   Alabama       01019  2011    3         0  85.736273     25819.0   \n",
      "\n",
      "    Income  HomePrice     ...      NoEnglish  InformationJobsPer  \\\n",
      "0  27807.0   137500.0     ...            0.0                 1.0   \n",
      "1  25937.0   175700.0     ...            0.0                 1.7   \n",
      "2  15715.0    91600.0     ...            0.0                 0.8   \n",
      "3  20878.0    87500.0     ...            0.0                 0.5   \n",
      "4  22529.0   111500.0     ...            0.0                 1.4   \n",
      "5  18583.0    68000.0     ...            0.0                 1.7   \n",
      "6  18044.0    74300.0     ...            0.0                 1.1   \n",
      "7  19529.0    99600.0     ...            0.0                 1.8   \n",
      "8  17515.0    82400.0     ...            0.0                 3.0   \n",
      "9  20788.0    96200.0     ...            0.1                 0.9   \n",
      "\n",
      "   ArmedForcesPer  Unemployed  TotalHouseholds  SameHouseResidence  \\\n",
      "0             1.7         4.8          19998.0             45300.0   \n",
      "1             0.2         4.6          70757.0            147609.0   \n",
      "2             0.0         6.5           9589.0             22270.0   \n",
      "3             0.0         5.5           7225.0             20493.0   \n",
      "4             0.1         5.4          20954.0             48985.0   \n",
      "5             0.0         8.4           3760.0              9071.0   \n",
      "6             0.0         6.8           8090.0             19193.0   \n",
      "7             0.2         6.9          45923.0             96878.0   \n",
      "8             0.0         8.6          13562.0             28858.0   \n",
      "9             0.1         7.3          11508.0             22888.0   \n",
      "\n",
      "   Annual Payroll  NumberofFirms  TractArea  PopDensity  \n",
      "0        276674.0          835.0    594.439   90.747747  \n",
      "1       1516145.0         4624.0   1589.808  112.921183  \n",
      "2        219989.0          501.0    884.877   31.129750  \n",
      "3         95812.0          285.0    622.583   36.534888  \n",
      "4        178005.0          690.0    644.807   88.615663  \n",
      "5             0.0          113.0    622.806   17.464507  \n",
      "6        143446.0          411.0    776.828   26.852791  \n",
      "7       1166450.0         2333.0    605.889  194.118065  \n",
      "8        194659.0          557.0    596.531   57.624834  \n",
      "9         99740.0          351.0    553.720   46.628260  \n",
      "\n",
      "[10 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(broadband.shape)\n",
    "print(broadband.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadband.to_csv('broadband.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
